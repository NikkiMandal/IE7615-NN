{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Problem_1\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(tf.__version__)    #YOUR TF VERSION CHECK\n",
        "print(tfds.__version__)  #YOUR TFDS VERSION CHECK\n",
        "\n",
        "# Load training and test data\n",
        "(ds_trn, ds_tst), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,  # 2. Shuffle training data\n",
        "    as_supervised=True,  # 3. Return (input, label) tuple\n",
        "    with_info=True,      # 4. Include dataset info\n",
        ")\n",
        "\n",
        "#  VERIFY THAT YOUR TENSOR SHAPES ARE (28, 28, 1)\n",
        "print(ds_trn)\n",
        "print(ds_tst)\n",
        "\n",
        "# NORMALIZE INPUT IMAGES\n",
        "def normalize_image(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "# Apply normalization function to training and testing datasets\n",
        "ds_trn = ds_trn.map(normalize_image)\n",
        "ds_tst = ds_tst.map(normalize_image)\n",
        "\n",
        "# Shuffle the dataset\n",
        "ds_trn_shuffled = ds_trn.shuffle(ds_info.splits['train'].num_examples)\n",
        "\n",
        "# Manually split the dataset into training and validation sets\n",
        "train_size = int(0.9 * ds_info.splits['train'].num_examples)\n",
        "ds_trn = ds_trn_shuffled.take(train_size)  # Take the first 90% for training\n",
        "ds_val = ds_trn_shuffled.skip(train_size)  # Skip the first 90% for validation\n",
        "\n",
        "# Batch and prefetch the datasets for better performance\n",
        "ds_trn = ds_trn.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_val = ds_val.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_tst = ds_tst.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(ds_trn)\n",
        "print(ds_tst)\n",
        "print(ds_val)\n",
        "\n",
        "# DEFINE NETWORK MODEL\n",
        "model_cnn = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),  # Max pooling layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),  # Fully connected layer FC1\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer FC2\n",
        "])\n",
        "\n",
        "# COMPILE NETWORK MODEL\n",
        "model_cnn.compile(optimizer=tf.keras.optimizers.Adam(),  # ADAM optimizer\n",
        "                  loss='sparse_categorical_crossentropy',  # Sparse categorical cross-entropy loss\n",
        "                  metrics=['accuracy'])  # Accuracy metric\n",
        "\n",
        "print(\"Training dataset size:\", len(ds_trn))\n",
        "print(\"Validation dataset size:\", len(ds_val))\n",
        "\n",
        "# TRAIN NETWORK\n",
        "history_cnn = model_cnn.fit(ds_trn, epochs=6, validation_data=ds_val)  # Training for 6 epochs with validation data\n",
        "\n",
        "# EVALUATE TEST SET PERFORMANCE\n",
        "test_loss, test_acc = model_cnn.evaluate(ds_tst)  # Evaluate the model on the test set\n",
        "print(f'Test accuracy: {test_acc}')  # Print the test accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY25duaboR8v",
        "outputId": "fac9e2d9-e201-4e85-ee7e-dd2163f5dca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "4.9.4\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "Training dataset size: 422\n",
            "Validation dataset size: 47\n",
            "Epoch 1/6\n",
            "422/422 [==============================] - 181s 414ms/step - loss: 0.1612 - accuracy: 0.9522 - val_loss: 0.0532 - val_accuracy: 0.9842\n",
            "Epoch 2/6\n",
            "422/422 [==============================] - 178s 416ms/step - loss: 0.0446 - accuracy: 0.9862 - val_loss: 0.0277 - val_accuracy: 0.9912\n",
            "Epoch 3/6\n",
            "422/422 [==============================] - 169s 396ms/step - loss: 0.0274 - accuracy: 0.9920 - val_loss: 0.0163 - val_accuracy: 0.9942\n",
            "Epoch 4/6\n",
            "422/422 [==============================] - 169s 396ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0137 - val_accuracy: 0.9952\n",
            "Epoch 5/6\n",
            "422/422 [==============================] - 167s 391ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0073 - val_accuracy: 0.9978\n",
            "Epoch 6/6\n",
            "422/422 [==============================] - 169s 395ms/step - loss: 0.0103 - accuracy: 0.9962 - val_loss: 0.0106 - val_accuracy: 0.9965\n",
            "79/79 [==============================] - 8s 103ms/step - loss: 0.0419 - accuracy: 0.9886\n",
            "Test accuracy: 0.9886000156402588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem_2\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(tf.__version__)    # YOUR TF VERSION CHECK\n",
        "print(tfds.__version__)  # YOUR TFDS VERSION CHECK\n",
        "\n",
        "# Load training and test data\n",
        "(ds_trn, ds_tst), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,  # Shuffle training data\n",
        "    as_supervised=True,  # Return (input, label) tuple\n",
        "    with_info=True,      # Include dataset info\n",
        ")\n",
        "\n",
        "#  VERIFY THAT YOUR TENSOR SHAPES ARE (28, 28, 1)\n",
        "print(ds_trn)\n",
        "print(ds_tst)\n",
        "\n",
        "# NORMALIZE INPUT IMAGES\n",
        "def normalize_image(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "# Apply normalization function to training and testing datasets\n",
        "ds_trn = ds_trn.map(normalize_image)\n",
        "ds_tst = ds_tst.map(normalize_image)\n",
        "\n",
        "# Shuffle the dataset\n",
        "ds_trn_shuffled = ds_trn.shuffle(ds_info.splits['train'].num_examples)\n",
        "\n",
        "# Manually split the dataset into training and validation sets\n",
        "train_size = int(0.9 * ds_info.splits['train'].num_examples)\n",
        "ds_trn = ds_trn_shuffled.take(train_size)  # Take the first 90% for training\n",
        "ds_val = ds_trn_shuffled.skip(train_size)  # Skip the first 90% for validation\n",
        "\n",
        "# Batch and prefetch the datasets for better performance\n",
        "ds_trn = ds_trn.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_val = ds_val.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_tst = ds_tst.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(ds_trn)\n",
        "print(ds_tst)\n",
        "print(ds_val)\n",
        "\n",
        "# DEFINE NETWORK MODEL (Simple deep MLP)\n",
        "model_mlp = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),  # Flatten input images\n",
        "    tf.keras.layers.Dense(300, activation='relu'),   # Hidden layer 1 with 300 units and ReLU activation\n",
        "    tf.keras.layers.Dense(100, activation='relu'),   # Hidden layer 2 with 100 units and ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer with softmax activation\n",
        "])\n",
        "\n",
        "# COMPILE NETWORK MODEL\n",
        "model_mlp.compile(optimizer=tf.keras.optimizers.Adam(),  # ADAM optimizer\n",
        "                  loss='sparse_categorical_crossentropy',  # Sparse categorical cross-entropy loss\n",
        "                  metrics=['accuracy'])  # Accuracy metric\n",
        "\n",
        "print(\"Training dataset size:\", len(ds_trn))\n",
        "print(\"Validation dataset size:\", len(ds_val))\n",
        "\n",
        "# TRAIN NETWORK\n",
        "history_mlp = model_mlp.fit(ds_trn, epochs=6, validation_data=ds_val)  # Training for 6 epochs with validation data\n",
        "\n",
        "# EVALUATE TEST SET PERFORMANCE\n",
        "test_loss, test_acc = model_mlp.evaluate(ds_tst)  # Evaluate the model on the test set\n",
        "print(f'Test accuracy: {test_acc}')  # Print the test accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEzW_wjaSZ2Q",
        "outputId": "ed25adfd-3d5b-47ce-e328-281cdf5197f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "4.9.4\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "Training dataset size: 422\n",
            "Validation dataset size: 47\n",
            "Epoch 1/6\n",
            "422/422 [==============================] - 13s 15ms/step - loss: 0.2854 - accuracy: 0.9188 - val_loss: 0.1324 - val_accuracy: 0.9602\n",
            "Epoch 2/6\n",
            "422/422 [==============================] - 12s 22ms/step - loss: 0.1112 - accuracy: 0.9665 - val_loss: 0.0764 - val_accuracy: 0.9758\n",
            "Epoch 3/6\n",
            "422/422 [==============================] - 11s 21ms/step - loss: 0.0739 - accuracy: 0.9774 - val_loss: 0.0483 - val_accuracy: 0.9862\n",
            "Epoch 4/6\n",
            "422/422 [==============================] - 11s 20ms/step - loss: 0.0524 - accuracy: 0.9840 - val_loss: 0.0388 - val_accuracy: 0.9883\n",
            "Epoch 5/6\n",
            "422/422 [==============================] - 8s 15ms/step - loss: 0.0401 - accuracy: 0.9878 - val_loss: 0.0289 - val_accuracy: 0.9915\n",
            "Epoch 6/6\n",
            "422/422 [==============================] - 9s 14ms/step - loss: 0.0332 - accuracy: 0.9899 - val_loss: 0.0311 - val_accuracy: 0.9892\n",
            "79/79 [==============================] - 1s 13ms/step - loss: 0.0800 - accuracy: 0.9745\n",
            "Test accuracy: 0.9745000004768372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem_3\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(tf.__version__)    # YOUR TF VERSION CHECK\n",
        "print(tfds.__version__)  # YOUR TFDS VERSION CHECK\n",
        "\n",
        "# Load training and test data\n",
        "(ds_trn, ds_tst), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,  # Shuffle training data\n",
        "    as_supervised=True,  # Return (input, label) tuple\n",
        "    with_info=True,      # Include dataset info\n",
        ")\n",
        "\n",
        "#  VERIFY THAT YOUR TENSOR SHAPES ARE (28, 28, 1)\n",
        "print(ds_trn)\n",
        "print(ds_tst)\n",
        "\n",
        "# NORMALIZE INPUT IMAGES\n",
        "def normalize_image(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "# Apply normalization function to training and testing datasets\n",
        "ds_trn = ds_trn.map(normalize_image)\n",
        "ds_tst = ds_tst.map(normalize_image)\n",
        "\n",
        "# Shuffle the dataset\n",
        "ds_trn_shuffled = ds_trn.shuffle(ds_info.splits['train'].num_examples)\n",
        "\n",
        "# Manually split the dataset into training and validation sets\n",
        "train_size = int(0.9 * ds_info.splits['train'].num_examples)\n",
        "ds_trn = ds_trn_shuffled.take(train_size)  # Take the first 90% for training\n",
        "ds_val = ds_trn_shuffled.skip(train_size)  # Skip the first 90% for validation\n",
        "\n",
        "# Batch and prefetch the datasets for better performance\n",
        "ds_trn = ds_trn.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_val = ds_val.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_tst = ds_tst.batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(ds_trn)\n",
        "print(ds_tst)\n",
        "print(ds_val)\n",
        "\n",
        "# DEFINE NETWORK MODEL (Single hidden-layer feedforward MLP)\n",
        "model_mlp_single = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),  # Flatten input images\n",
        "    tf.keras.layers.Dense(100, activation='relu'),    # Hidden layer with 100 units and ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')   # Output layer with softmax activation\n",
        "])\n",
        "\n",
        "# COMPILE NETWORK MODEL\n",
        "model_mlp_single.compile(optimizer=tf.keras.optimizers.Adam(),  # ADAM optimizer\n",
        "                          loss='sparse_categorical_crossentropy',  # Sparse categorical cross-entropy loss\n",
        "                          metrics=['accuracy'])  # Accuracy metric\n",
        "\n",
        "print(\"Training dataset size:\", len(ds_trn))\n",
        "print(\"Validation dataset size:\", len(ds_val))\n",
        "\n",
        "# TRAIN NETWORK\n",
        "history_mlp_single = model_mlp_single.fit(ds_trn, epochs=6, validation_data=ds_val)  # Training for 6 epochs with validation data\n",
        "\n",
        "# EVALUATE TEST SET PERFORMANCE\n",
        "test_loss, test_acc = model_mlp_single.evaluate(ds_tst)  # Evaluate the model on the test set\n",
        "print(f'Test accuracy: {test_acc}')  # Print the test accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvpMgdxnUjGT",
        "outputId": "623623db-e61c-4bc2-e45e-47e9ed8a6eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "4.9.4\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "Training dataset size: 422\n",
            "Validation dataset size: 47\n",
            "Epoch 1/6\n",
            "422/422 [==============================] - 10s 10ms/step - loss: 0.3955 - accuracy: 0.8927 - val_loss: 0.2035 - val_accuracy: 0.9400\n",
            "Epoch 2/6\n",
            "422/422 [==============================] - 7s 13ms/step - loss: 0.1792 - accuracy: 0.9499 - val_loss: 0.1441 - val_accuracy: 0.9580\n",
            "Epoch 3/6\n",
            "422/422 [==============================] - 7s 12ms/step - loss: 0.1339 - accuracy: 0.9621 - val_loss: 0.1140 - val_accuracy: 0.9665\n",
            "Epoch 4/6\n",
            "422/422 [==============================] - 6s 10ms/step - loss: 0.1064 - accuracy: 0.9694 - val_loss: 0.0952 - val_accuracy: 0.9718\n",
            "Epoch 5/6\n",
            "422/422 [==============================] - 6s 10ms/step - loss: 0.0880 - accuracy: 0.9746 - val_loss: 0.0714 - val_accuracy: 0.9803\n",
            "Epoch 6/6\n",
            "422/422 [==============================] - 6s 10ms/step - loss: 0.0750 - accuracy: 0.9791 - val_loss: 0.0636 - val_accuracy: 0.9805\n",
            "79/79 [==============================] - 1s 12ms/step - loss: 0.0936 - accuracy: 0.9724\n",
            "Test accuracy: 0.9724000096321106\n"
          ]
        }
      ]
    }
  ]
}